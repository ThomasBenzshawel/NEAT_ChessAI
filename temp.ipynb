{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import random\n",
    "\n",
    "@dataclass\n",
    "class Layer:\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Input(Layer):\n",
    "    out_features: int\n",
    "\n",
    "@dataclass\n",
    "class Dense(Layer):\n",
    "    out_features: int\n",
    "    learning_rate: float\n",
    "\n",
    "@dataclass\n",
    "class Conv(Layer):\n",
    "    kernel_size: tuple[int,int]\n",
    "    learning_rate: float\n",
    "\n",
    "@dataclass\n",
    "class Attention(Layer):\n",
    "    learning_rate: float\n",
    "\n",
    "@dataclass\n",
    "class BatchNorm(Layer):\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class SkipConn(Layer):\n",
    "    from_offset: int\n",
    "    learning_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [\n",
    "    Input(2),\n",
    "    Dense(2, .3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_torch_model(layers: list[Layer]):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import random\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, in_features, out_features, learning_rate):\n",
    "        self.layer = nn.Linear(in_features, out_features)\n",
    "        self.actication = nn.functional.relu\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.layer.bias += self.learning_rate * torch.randn_like(self.layer.bias)\n",
    "            self.layer.weight += self.learning_rate * torch.randn_like(self.layer.weight)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.actication(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Input(4), Dense(2), Skip(-2), Dense(6), Attention(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, prior, out_features):\n",
    "        self.prior = prior\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def update(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.prior(x)\n",
    "\n",
    "    def iter(self, ret=None):\n",
    "        if ret is None:\n",
    "            ret = [self]\n",
    "        return self.prior.iter() + ret\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "    def __init__(self, out_features):\n",
    "        super().__init__(None, out_features)\n",
    "\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "\n",
    "    def iter(self):\n",
    "        return [self]\n",
    "    \n",
    "class NumOpLayer(Layer):\n",
    "    def __init__(self, prior, out_features, learning_rate=None):\n",
    "        super().__init__(prior, out_features)\n",
    "        if learning_rate is None:\n",
    "            learning_rate = random()\n",
    "        self.layer = nn.Linear(self.prior.out_features, out_features)\n",
    "        self.actication = F.relu\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.layer.bias += self.learning_rate * torch.randn_like(self.layer.bias)\n",
    "            self.layer.weight += self.learning_rate * torch.randn_like(self.layer.weight)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.actication(self.layer(self.prior(x)))\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, prior, out_features, learning_rate=None):\n",
    "        super().__init__(prior, out_features)\n",
    "        if learning_rate is None:\n",
    "            learning_rate = random()\n",
    "        self.layer = nn.Linear(self.prior.out_features, out_features)\n",
    "        self.actication = F.relu\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.layer.bias += self.learning_rate * torch.randn_like(self.layer.bias)\n",
    "            self.layer.weight += self.learning_rate * torch.randn_like(self.layer.weight)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.actication(self.layer(self.prior(x)))\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, prior, out_features, learning_rate=None):\n",
    "        super().__init__(prior, out_features)\n",
    "        if learning_rate is None:\n",
    "            learning_rate = random()\n",
    "        self.layer = nn.Attention(prior.out_features, 1)\n",
    "        self.actication = F.relu\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.layer.bias += self.learning_rate * torch.randn_like(self.layer.bias)\n",
    "            self.layer.weight += self.learning_rate * torch.randn_like(self.layer.weight)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.actication(self.layer(self.prior(x)))\n",
    "\n",
    "# class Layer():\n",
    "#     def __init__(self, prior, out_features, learning_rate):\n",
    "#         self.layer = nn.Linear(in_features, out_features)\n",
    "#         self.actication = nn.functional.relu\n",
    "\n",
    "#         self.prior = prior\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.out_features = out_features\n",
    "\n",
    "#     def update(self):\n",
    "#         with torch.no_grad():\n",
    "#             self.layer.bias += self.learning_rate * torch.randn_like(self.layer.bias)\n",
    "#             self.layer.weight += self.learning_rate * torch.randn_like(self.layer.weight)\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         return self.actication(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.MultiheadAttention(4, 1, batch_first=True)\n",
    "conv = nn.Conv1d(3, 1, 2)\n",
    "# conv2 = nn.Conv2d(2, 1, 2)\n",
    "lin = nn.Linear(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6899, -0.3772,  0.4068, -0.5882],\n",
      "        [ 0.3314, -0.2893,  0.2962,  0.3016],\n",
      "        [ 0.3314, -0.2893,  0.2962,  0.3016]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "tensor([[-0.0205, -0.0167, -0.0051,  0.0785],\n",
      "        [-0.0085, -0.0100, -0.0147,  0.0618],\n",
      "        [-0.0085, -0.0100, -0.0147,  0.0618]], grad_fn=<SqueezeBackward1>) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2044, -0.2029, -0.2248]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = lin(torch.Tensor([\n",
    "    # [\n",
    "    # [0, 1, 0],\n",
    "    # [1, 2, 0]\n",
    "    # ],\n",
    "    # [\n",
    "    [1, 0, 1],\n",
    "    [1, 2, 1],\n",
    "    [1, 2, 1]\n",
    "    # ]\n",
    "]))\n",
    "print(x,'\\n')\n",
    "x, _ = attn(x, x, x, need_weights=False)\n",
    "x\n",
    "print(x, '\\n')\n",
    "conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractLayer:\n",
    "    def __init__(self, prior:AbstractLayer, out_features:int, learning_rate:float):\n",
    "        self.prior = propr\n",
    "        self.out_features = out_features\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return prior(x)\n",
    "\n",
    "class Input(AbstractLayer):\n",
    "    def __init__(self, out_features:int):\n",
    "        super().__init__(out_features, 0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "\n",
    "class Dense(AbstractLayer):\n",
    "    def __init__(self, prior:AbstractLayer, out_features:int, learning_rate:float):\n",
    "        super().__init__(prior, out_features, learning_rate)\n",
    "        self.layer = nn.Linear(prior.out_features, out_features)\n",
    "\n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.layer.bias += self.learning_rate * torch.randn_like(self.layer.bias)\n",
    "            self.layer.weight += self.learning_rate * torch.randn_like(self.layer.weight)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.layer(super().__call__(x))\n",
    "\n",
    "class Conv(AbstractLayer):\n",
    "    def __init__(self, prior:AbstractLayer, kernel_size:int, channels:int, learning_rate:float):\n",
    "        \n",
    "        self.layer = nn.Conv1d()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w\n",
    "\n",
    "conv_output_shape((2,4), conv.kernel_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1052,  0.2616, -0.0619]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[1.]]], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3]).unsqueeze(0).unsqueeze(0)\n",
    "nn.MultiheadAttention(3, 1)(a,a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(2)\n",
    "x = Dense(x, 3)\n",
    "x = Dense(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Input at 0x7f09901b2b70>,\n",
       " <__main__.Dense at 0x7f09901b36b0>,\n",
       " <__main__.Dense at 0x7f09901b3470>]"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x.iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0547, 0.3688, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(torch.Tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Layer(1,2,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.4307, 4.3782], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(torch.Tensor([5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = nn.Linear(1,2,bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1076, 3.0439], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([0])\n",
    "dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dense.bias += torch.randn_like(dense.bias)\n",
    "    dense.weights += torch.randn_like(dense.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.relu(torch.Tensor([-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.9067]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.6769], requires_grad=True)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dense.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
